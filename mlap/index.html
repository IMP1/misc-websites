<!DOCTYPE html>
<html>
	<head>
		<title>MLAP Revision Notes</title>
		<meta charset="UTF-8">
		<link rel="stylesheet" type="text/css" href="../grat/style.css">
	</head>
	<body>
		<h1>MLAP Notes</h1>
		<section>
			<h2>Suresh</h2>
			<section>
				<h3>Linear Regression</h3>
				<section>
					<h4>Error</h4>
					The best fit of the equation is the one with the least error.
					<section>
						<span class="tech-term">Absolute Loss Error</span> = | y - f(x) |
						<br>
						<span class="tech-term">Squared Loss</span> = (y - f(x))<sup>2</sup>
						<br>
						<span class="tech-term">&epsilon;-sensitive Loss</span> = max(|y - f(x)| - &epsilon;, 0).
						<br>
						This only considers errors greater than &epsilon;.
					</section>
					Generally least squares is used with linear regression.
				</section>
			</section>
			<section>
				<h3>Logistic Regression</h3>
				<p>
					sigmoid(x) = 1 / (1 + exp(-x))
					<br>
					The sigmoid function gives a smoother differentiable soft decision boundary, unlike the step function, which gives a hard decision boundary.
				</p>
			</section>
			<section>
				<h3>Regularisation</h3>
				<p>
					arg<sub>&theta;</sub>max L(&theta;, X) = arg<sub>&theta;</sub>max log(L(&theta;, X)) because log is monotonic for positive values.
				</p>
				<p>
					Model<sub>MAP</sub>(Data) = arg<sub>Model</sub>max p(Data|Model)p(Model)
					<br>
					= arg<sub>&theta;</sub>max[ &Pi; p(Y = c|X, &theta;) ] e <sup>[-(1-&lambda;)/&lambda;] &Sigma;|&theta;|<sup>p</sup></sup>
				</p>
				<section>
					<h4>Lasso Regularisation</h4>
					p = 1
				</section>
				<section>
					<h4>Ridge Regularisation</h4>
					p = 2
				</section>
			</section>
		</section>
		<section>
			<h2>James</h2>
			<section>
				<h3>Bayes Networks</h3>
				<p>Directed acyclic graph.</p>
				<section>
					<h4>Collider</h4>
					<p>Node has two incoming edges.</p>
				</section>
				<section>
					<h4>Immoraility</h4>
					<p>Node has two unmarried parents.</p>
				</section>
			</section>
			<section>
				<h3>Hidden Markov Model</h3>
				<p>
					Time Series, with directed arrows and a hidden layer of nodes.
					Each hidden variable usually has a visibile variable.
				</p>
			</section>
			<section>
				<h3>Markov Chain</h3>
				<p>
					Time Series, but without hidden nodes/variables.
					Can have N-Order Markov chains, where the last N nodes are linked to a node.
				</p>
			</section>
			<section>
				<h3>Maximum Likelihood Estimates</h3>
			</section>
			<section>
				<h3>Inference Problems</h3>
				<p>Used to find the hidden values in a Hidden Markov Model.</p>
				<section>
					<h4>Filtering</h4>
					<p>Present</p>
				</section>
				<section>
					<h4>Prediction</h4>
					<p>Future</p>
				</section>
				<section>
					<h4>Smoothing</h4>
					<p>Past</p>
				</section>
			</section>
			<section>
				<h3>Naive Bayes</h3>
				<p>
					(Usually binary)
					<br>
					For each class:
					<br>
					p(c = 0) count up the 1 values for the varaibles where c = 0
					<br>
					p(c = 1) count up the 1 values for the varaibles where c = 0
				</p>
			</section>
		<section>
			<h2>Edwin</h2>
		</section>
	</body>
</html>